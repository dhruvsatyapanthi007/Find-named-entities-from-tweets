{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from ast import literal_eval\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "            if token.startswith(\"@\"):\n",
    "                token=\"<USR>\"\n",
    "            elif token.startswith(\"http://\") or token.startswith(\"https://\"):\n",
    "                token=\"<URL>\"\n",
    "            \n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "            \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = read_data('data/train.txt')\n",
    "X_val, y_val = read_data('data/validation.txt')\n",
    "X_test, y_test = read_data('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsets={}\n",
    "X=X_train\n",
    "for x in X:\n",
    "    for word in x:\n",
    "        wordsets[word]=1\n",
    "X=X_val\n",
    "for x in X:\n",
    "    for word in x:\n",
    "        wordsets[word]=1\n",
    "X=X_test\n",
    "for x in X:\n",
    "    for word in x:\n",
    "        wordsets[word]=1\n",
    "wordsets=list(wordsets.keys())\n",
    "NX=len(wordsets)\n",
    "wordsets={}\n",
    "y_train1=y_train\n",
    "for tag in y_train1:\n",
    "    for word in tag:\n",
    "        wordsets[word]=1\n",
    "y_train1=y_val\n",
    "for tag in y_train1:\n",
    "    for word in tag:\n",
    "        wordsets[word]=1\n",
    "Ny=len(wordsets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X_train+X_val+X_test\n",
    "ml=max([len(x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer=Tokenizer(num_words=NX)\n",
    "tokenizer.fit_on_texts(list(X_train)+list(X_val)+list(X_test))\n",
    "sequences=tokenizer.texts_to_sequences(list(X_train)+list(X_val))\n",
    "X_new=pad_sequences(sequences,maxlen=ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trbl=10\n",
    "num_layers=3\n",
    "state_size=128\n",
    "num_classes=100\n",
    "hidden_size=100\n",
    "p=0.5\n",
    "num_features=100\n",
    "max_length=max(max([len(x) for x in X_train]),max([len(x) for x in X_val]),max([len(x) for x in X_test]))\n",
    "batch_size=100\n",
    "num_epochs=1000\n",
    "batches=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer2=Tokenizer(num_words=Ny)\n",
    "tokenizer2.fit_on_texts(list(y_train)+list(y_val))\n",
    "sequences2=tokenizer2.texts_to_sequences(list(y_train)+list(y_val))\n",
    "labels=[]\n",
    "y=list(y_train)+list(y_val)\n",
    "for tag in sequences2:\n",
    "    t=np.zeros(Ny)\n",
    "    for wordid in tag:\n",
    "        t[sequences2[wordid]]=1\n",
    "    labels.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer=Tokenizer(num_words=NX+1)\n",
    "tokenizer.fit_on_texts(list(X_train)+list(X_val)+list(X_test))\n",
    "sequences=tokenizer.texts_to_sequences(list(X_train)+list(X_val))\n",
    "ml=max([len(x) for x in sequences])\n",
    "X_train_new=pad_sequences(sequences,maxlen=ml)\n",
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=NX,output_dim=num_features,input_length=ml,trainable=True,mask_zero=0))\n",
    "model.add(LSTM(units=state_size,dropout=0.5,recurrent_dropout=0.5))\n",
    "model.add(Dense(num_classes,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(X_train_new,np.array(labels),validation_split=0.2,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
